# History of APIs
APIs are not new, and are something that emerged in the earliest days of digital computing back in the 1950s, but are something that has evolved to meet the needs of a variety of business sectors, and have seen a massive standardization and growth as a result of the web and emergence of mobile networks.

## Planting Early Seeds
​​The compute and network origins of APIs begins in the early days of computer technology, realizing that this new technology has the greatest impact when used in a collaborative way.

- **SAGE**: In 1954, the Semi-Automatic Ground Environment (SAGE) began its six-year development to be used as an early warning air defense system. SAGE was developed by Massachusetts Institute of Technology (MIT) with some funding from the United States Air Force. A SAGE center could track up to 400 airplanes, using flight plans to differentiate between friendly and enemy aircraft. There were 23 SAGE centers, and each required at least 100 operators. The centers were able to communicate over telephone lines using modems, which started to become commercially available as part of the SAGE project. The first SAGE center came online in 1959.
- **SABRE**: In 1964, IBM built upon SAGE to develop Semi-Automatic Business Research Environment (SABRE), an air travel reservation system for American Airlines. This system connected 2000 air terminals across 60 cities by telephone lines. SABRE is one of the earliest examples of a real-time operating system (RTOS). An RTOS is an application designed to process data in real time at a consistent pace.
- **ARPANET**: The early stages of the Advanced Research Projects Agency Network (ARPANET) began in 1966, developed by the United States Defense Advanced Research Projects Agency (DARPA). Larry Roberts and Thomas Marill created the first wide-area network connection, linking the TX-2 computer at MIT to the AN/FSQ-32 computer in Santa Monica over a telephone line. The slow speeds, cost, and inefficiency of sending data across telephone lines led Larry Roberts to incorporate packet switching, a concept introduced by Donald Davies at the 1967 Symposium on Operating Systems Principles, into ARPANET. ARPANET was the first large-scale network to use packet switching, which groups data into packets and sends them across a digital network. A packet includes a header, which the networking hardware uses to direct the packet, and a payload, which contains the packet's main message.

## Laying the Foundation
By the 1970s the earliest networks were being established, resulting in the need to share files, communicate via messages, and better explore how early APIs can be used for business.

- **FTP**: In 1971, an MIT student named Abhay Bhushan published RFC 114 with the original specification for the File Transfer Protocol (FTP). FTP built upon earlier protocols such as Telnet. While Telnet allowed operators to transfer documents between machines, it did not account for differences in architecture and operating systems, so it was difficult to use. FTP introduced a standardized way to send files and messages between computers, acting as an early form of email. In 1980, a revision to FTP that allowed operators to send and receive files using a more secure TCP/IP connection was published in RFC 765. FTP was revised again in 1985 in RFC 959, adding support for several new commands. This version of FTP is still in use today, but in 2021, several web browsers removed support for FTP in favor of more secure file transfer standards.
- **EDI**: In the 1960s, electronic data interchange (EDI) was introduced. While managing shipment supply chains in the United States Army, Ed Guilbert developed the EDI process by digitizing shipping manifests. Using EDI, a business can exchange structured data in batches with external partners. These batches of bundled information conform to industry-specific messaging standards, which makes it possible to efficiently exchange large amounts of information, including purchase orders, invoices, reservations, or shipment statuses. The first EDI messages were sent in 1965, and in 1968 the Transportation Data Coordinating Committee (TDCC) was formed to develop EDI standards. With the help of Ed Guilbert, the TDCC published the first set of EDI standards in 1975. EDI continues to be used in industries such as automotive, manufacturing, logistics, and utilities.
- **Email**: In 1961, MIT introduced the Compatible Time-Sharing System (CTSS), which was the first system that allowed multiple users to remotely access a centralized server and share files. Operators could send messages to one another by sending text files over FTP, and by 1965 they implemented a `MAIL` command that saved the messages to a user's mailbox. Other groups of operators started to develop similar messaging systems around the same time, including ARPANET. In 1971, Ray Tomlinson introduced the idea of using the `@` symbol to direct a message to a user on a specific ARPANET system. In 1982, the Simple Mail Transfer Protocol (SMTP), based on Ray Tomlinson's work, was created to standardize the way servers send and receive electronic mail. SMTP replaced FTP as the protocol for sending and receiving mail. In 1988, Microsoft Mail was released for Mac OS, allowing users to send messages on the first commercial email application. In 1992, Multipurpose Internet Mail Extension (MIME) was introduced, which allowed users to send images, audio files, and videos as email attachments. In the mid-1990s, Internet service providers (ISPs) like America Online (AOL) began bundling webmail into their service, and several free webmail services such as Hotmail and Yahoo! Mail appeared during this time as well. Gmail was launched in 2001 as an internal tool for Google employees, and it was made available as an invite-only beta service in 2004 until its public release in 2007.

## Conducting Business
The business value of early APIs was clear, there was just much more work to be done when it comes to standardizing how we would be able to communicate via the new networks we had developed.

- **CORBA**: In the early 1990s, developers found it difficult to manage communication between applications with different operating systems, hardware, and programming languages. In 1991, the Object Management Group (OMG) introduced the Common Object Request Broker Architecture (CORBA), a middleware specification that allowed applications programmed in C to communicate with each other using object request brokers (ORBs) and application programming interfaces (APIs). An ORB exists between the server and the client, where it listens for requests, locates objects, and returns results for systems regardless of hardware or location. CORBA 2.0 was released in 1996, adding support for C++ and defined a standard protocol that improved interoperability between applications. Java language mapping was added two years later. As the Internet grew larger and more complex in the late 1990s, many developers found that it was difficult to build large-scale applications with CORBA, and its performance was unable to keep up with higher-speed networks.
- **XML**: In the 1970s, the Standard Generalized Markup Language (SGML) was developed at IBM as a standard for structured markup languages. It was designed to efficiently handle text files that were thousands of pages long, and included various markup tags that specified the document type such as `<article>`, sections such as `<chapter>`, and formatting such as `<emphasis>`. SGML was very powerful, but difficult to use. When Dan Connolly joined the World Wide Web Consortium (W3C) in 1995, he added SGML to the list of activities that W3C should work on. In 1996, an 11-person working group began meeting remotely to develop a lighter version of SGML, which became the Extensible Markup Language (XML) in 1998. XML is designed to be both human-readable and machine-readable, using many SGML features in a more streamlined way. XML was quickly adopted by web developers, who were able to create websites that were more interactive than those programmed in HTML. XML also allows developers to define information about the document, which can make it easier for search engines to return useful results.
- **SOA**: In the 1990s and earlier, client-server was the dominant architecture for applications. Developers had to configure their applications to communicate directly with any other systems. Around 1998, the idea of service-oriented architecture (SOA) started to emerge. This model separated the concept of services from other applications. A service represents a self-contained, repeatable task that is typically a specific business function. For example, processing a loan application, pulling a consumer's credit score, or checking the weather are tasks that might be handled by a service. Services can communicate with applications, servers, and other services over a messaging system called the enterprise service bus (ESB).

As the web emerged, many industry forces came together to formalize XML specifications in service of a SOA vision for industry, but the simpler, more low-cost, and increasingly ubiquitous web would provide a much more powerful approach to delivering the digital resources and capabilities we would need.
